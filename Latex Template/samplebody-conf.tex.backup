\section{Introduction}

This means the model does not have to explicitly
store gigantic phrase tables and language models
as in  the case  of standard  MT; hence,  NMT has
a small memory footprint.   Lastly,  implementing
NMT decoders is easy unlike the highly intricate
decoders in standard MT.

\section{Neural Machine Translations}
NMT is an approach to machine translation that uses a large neural network. It departs from phrases-based statistical approaches that use separately engineerd subcomponents.Neural machine translation (NMT) is not a drastic step beyond what has been traditionally done in statistical machine translation (SMT). MNT starts emitting one target word a time  as illustrated in. NMT is often a large neural network that is trained in an end-to-end fashion and has the abil-ity to generalize well to very long word sequences.

\subsection{Encoder and decoder arquitecture}

A  neural  machine  translation  system  is  a  neural
network that directly models the conditional prob-ability $p(y|x)$ of translating a source sentence,$x_{1}$,...,$x_{n}$  to a target sentence $y_{1}$,...,$y_{n}$ A basic form of NMT consists of two components: $(a)$ an which computes a representations for each source sentence and $(b)$ a decoder which generates one target word at a time and hence decomposes the conditional
probability as:

% Introduce a new line

\begin{gather*}
  \log p(y|x) = \sum_{j=1}^{m}log p(y_{j}|y<j,s)
\end{gather*}

\begin{center}
\includegraphics[width=0.7\columnwidth]{nmt} \\% Example image
    Figura 1. Encoder-decoder architecture
\end{center}

\subsection{Long short term memory}
We consider as examples a deep multi-layer RNN which is unidirectional and uses LSTM as a recurrent unit. We show an example of such a model in Figure 2. In this example, we build a model to translate a source sentence "I am a student" into a target sentence "Je suis Ã©tudiant". At a high level, the NMT model consists of two recurrent neural networks: the encoder RNN simply consumes the input source words without making any prediction; the decoder, on the other hand, processes the target sentence while predicting the next words.


\begin{gather*}
  f_{t} = \sigma(W_{f}[h_{t-1},x_{t}]+b_{f}) \\
  i_{t} = \sigma(W_{i}[h_{t-1},x_{t}]+b_{i}) \\
  C_{t} = tanh(W_{c}[h_{t-1},x_{t}]+b_{c})   \\
\end{gather*}


\begin{center}
\includegraphics[width=0.7\columnwidth]{lstm} \\% Example image
    Figura 2. Long short-term memory
\end{center}

\section{Attention based models}

Our various  attention-based  models  are classifed
into two broad categories, global and local
. These classes differ in terms of whether the attention
is placed on all source positions or on only a few
source  positions.

Common to these two types of models is the fact
that at each time step $t$ in the decoding phase, both
approaches first take as input the hidden state $h_{t}$
at the top layer of a stacking LSTM. The goal is
then to derive a context vector $c_{t}$ that captures rel-evant source-side  information  to help predict  the current target word
$y_{t}$. While these models differ in how the context vector $c_{t}$
is derived, they share the same subsequent steps.

Specifically, given the target hidden state $h_{t}$
and the  source-side  context  vector $c_{t}$
,  we  employ  a simple concatenation  layer to combine the infor-mation 
from both vectors to produce an attentional hidden state as follows:

\begin{gather*}
  \widetilde{h_{t}} = tanh(W_{c}[c_{t},h_{t}])
\end{gather*}

The attentional vector $h_{t}$ is then fed through the
softmax  layer  to  produce  the  predictive  distribu-
tion formulated as:

\begin{gather*}
  p(y_{t}|y_{<t},x) = softmax(W_{s}\widetilde{h_{t}})
\end{gather*}

\subsection{Global attention}
The idea of a global attentional  model is to con-
sider all the hidden states of the encoder when de-
riving  the  context  vector $c_{t}$. In this  model  type,
a variable-length alignment vector $a_{t}$, whose size
equals the number of time steps on the source side,
is derived by comparing the current target hidden state
$h_{t}$ with each source hidden state $h_{s}$.

\begin{gather*}
  a_{t}(s) = align(h_{t},\bar{h_{s}})\\
           = \frac{ exp(score(h_{t},\bar{h_{s}})) }{ \sum_{s_{p}}exp(score(h_{t},h_{s})) }
\end{gather*}

\begin{center}
\includegraphics[width=0.7\columnwidth]{global} \\% Example image
    Figura 3. Global attentional model - at each time step t, the models infers a variable-length
    alignment weigth vector $a_{t}$ based on the current target state $h_{t}$ and all source states
    $\bar{h_{s}}$. A global context vector $c_{t}$ is then computed as the weighted average, according to $a_{t}$, over all the source states.
\end{center}

Here, score is referred as a \textit{content-based} function for which we consider three different alternatives:

\begin{gather*}
    score(h_{t},\bar{h_{s}}) =  \bar{h_{t}}^T \bar{h_{s}}       \\
    score(h_{t},\bar{h_{s}}) =  \bar{h_{t}}^T W_{a}\bar{h_{s}}   \\ 
    score(h_{t},\bar{h_{s}}) = \bar{v_{t}}^T Tanh( W_{a}[h_{t}:\bar{h_{s}}] )  \\
\end{gather*}

Besides alignment vector as weigths, the context vector $c_{t}$ is computed as the weight average over all the source
hidden states.


% \bar{q} 

\subsection{Local attention}
The global attention has a drawback that it has to
attend to all words on the source side for each tar-get word, which is expensive and can potentially
render it impractical to translate longer sequences, e.g.,  paragraphs  or  documents.   To  address  this
deficiency,  we propose  a local attentional  mech-anism that chooses to focus only on a small subset
of the source positions per target word. \\

This model takes inspiration from the tradeoff between the soft and hard attentional models proposed
by Xu et al.(2015) to tackle the image caption generation task. In their work, soft attention refers
to the global attention approach in which weights are placed "softly" over all patches in the source
image. The hard attention, on the order hand, selects on patch of the image to attend to a time.



\subsection{Input-feeding Approach}

In  our  proposed  global  and  local  approaches,
the attentional decisions are made independently,
which  is suboptimal.   Whereas,  in  standard  MT,
a coverage set  is  often  maintained  during  the
translation process to keep track of which source
words  have  been  translated.   Likewise,  in  atten-
tional NMTs, alignment decisions should be made
jointly  taking  into  account  past  alignment  infor-
mation.    To  address  that,  we  propose  an
input-feeding approach  in which attentional  vectors
$\bar{h_{t}}$ are concatenated with inputs at the next time
The  effects  of  hav-ing  such  connections  are  two-fold:  (a)  we  hope
to make the model fully aware of previous align-ment choices  and (b) we create a very deep net-
work spanning both horizontally and vertically.

\section{Experiments}

For this Section, we used the base RNN provided by Google's Library Tensorflow with a numerous changes in variables
always looking for achiving the best metrics that we could. \\

Our main metrics were perplexity and Bilingual Evaluation Understudy Score (Bleu) Score. Perplexity is a measurement of how well a probability distribution
predicts a sample. It is given by:

\begin{gather*}
2^{H(p)} = 2^{-\sum_{x} p(x)log_{2}p(x)}
\end{gather*}

where H(p) is the entropy of the distribution.\\


So, we may note that the lower the perplexity we get the better our model. But we can not rely on perplexity alone
because models can get low perplexity values but that does not mean they get good results. For this, we
take into account Bleu Score.\\

Bleu Score is more about the language and it is strongly correlated with ngrams. This metric is actually an
algorithm for evaluating the quality of text which has been machine-translated from one natural language
to another. The score is calculated for every sentence by compraing them with a set of good quality reference
translations. The scores are values between 0 and 1, where 1 means the same referenced sentence. Typically these
values are multiplied by 10 (like Google) or 100 (This work).

\subsection{Training Details}
The dataset used was the European Parliament Parallel Corpus, presented by Philipp Koehn \cite{europarl}.
This dataset is not processed for quick evaluation, it needed some preprocess work. More, we generated
our own vocabulary files. These vocabularies were created using 15k Top Frequent 1-grams present in both languages
databases with the help of python library Nltk.\\

In addition, We run NMT Tensorflow code on Intel Core i7-4770 CPU, 8 Gb RAM with a GPU GeForce GTX 750 Ti / 2 Gbs GPU' memory Machine.\\

Take into account that even with better characteristics training these types of models need several hours or
days to complete. We recommend running with tensorflow-GPU configuration and use GPU's with at least 8 Gbs of memory.\\

Next, we present our different trained models and the values we used.

\begin{table*}[h]
  \caption{Parameters on different Trained Models}
  \label{tab:1}
\begin{tabular}{@{}cccccccc@{}}
\toprule
Model\_Name & Num\_Units & Num\_Layers & Dropout & Unit\_Type & Attention\_Model & Train\_Steps & Batch \\ \midrule
nmt1\_en-es & 128        & 2           & 0.2     & lstm       & scaled\_luong    & 12,000       & 128   \\
nmt2\_en-es & 128        & 4           & 0.2     & lstm       & scaled\_luong    & 40,000       & 128   \\
nmt3\_en-es & 128        & 2           & 0.2     & lstm       & scaled\_luong    & 35000        & 128   \\
nmt4\_en-es & 128        & 3           & 0.2     & lstm       & scale\_luong     & 100,000      & 128   \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{English-Spanish Results}

On table \ref{tab:1}, we present our actual results on our trained models. Next, some sentences translated using our models.


\begin{table*}
  \caption{Perplexity and Bleu Scores on Test Sentences}
  \label{tab:2}
\begin{tabular}{@{}cccccccc@{}}
\toprule
Model\_Name & train\_sentences & src\_vocab\_sz & tgt\_vocab\_sz & wps     & training\_time & test\_ppl & test\_bleu \\ \midrule
nmt1\_en-es & 400,000          & 15,281         & 13,306         & 30.8  K & 50 min         & 18.63     & 12.6       \\
nmt2\_en-es & 400,000          & 15,778         & 15,488         & 22.5 K  & $\sim$ 4h      & 15.4      & 13.7       \\
nmt3\_en-es & 150,000          & 15,617         & 14,517         & 28.2 K  & 2h 40 min      & 10.21     & 15.9       \\
nmt4\_en-es & 400,000          & 15,617         & 14,517         & 25.8 K  & $\sim$ 8h      & 8.23      & 19.6       \\ \bottomrule
\end{tabular}
\end{table*}

\subsection{Sample Translations}
On table \ref{tab:3} we can see some translations made by our trained models. In the next section we analize
some characteristics of them. Note that special character word <unk> is present when model can not decide
what word it is. We assume this happens for a variety of reasons. We can mention lack of vocabulary or
not enough training as ones of this reasons.

We only show results on models 3 and 4, which got our best Bleu Scores.

\begin{table*}
  \caption{Sample Translations on Test(unseen) Sentences}
  \label{tab:3}
\begin{tabular}{cp{15cm}}
\toprule
Model\_Name & Sentences                                                                                        \\ \midrule
src:        & The proposal by Mrs Malliori is approved and her report will be put to the vote without a debate. \\
ref:        & Queda aprobada la propuesta de la Sra, Malliori y su informe se votarÃ¡ sin debate.               \\
nmt3\_en-es & La propuesta de la <unk> <unk> se <unk> y su informe se <unk> a la votaciÃ³n sin un <unk>          \\
nmt4\_en-es & La propuesta de la <unk> <unk> se aprueba y su informe se someterÃ¡ a votaciÃ³n sin un <unk>       \\ \midrule

src:        & The issue is: which exact text will best achieve that? . \\
ref:        & La cuestiÃ³n es esta: Â¿exactamente, quÃ© texto lo lograrÃ¡ mejor? .              \\
nmt3\_en-es & La cuestiÃ³n <unk> que <unk> el texto <unk> <unk> <unk> .         \\
nmt4\_en-es & El tema <unk> que el texto exacto se <unk>       \\ \midrule

src:        & Citizens must be able to identify which areas can be regulated by the European Union and which cannot. \\
ref:        & Los ciudadanos deben poder saber quÃ© cuestiones puede regular la UniÃ³n Europea y cuÃ¡les no.               \\
nmt3\_en-es & Los ciudadanos deben poder identificar las zonas que pueden regularse por la UniÃ³n Europea y que <unk>          \\
nmt4\_en-es & Los ciudadanos deben poder identificar las zonas que pueden regularse por la UniÃ³n Europea y que <unk>       \\ \midrule

src:        & That is my main concern in all this, and there are signs that this is again happening already. \\
ref:        & Esa es mi mayor preocupaciÃ³n en este contexto, y hay seÃ±ales de que esto ya estÃ¡ ocurriendo.               \\
nmt3\_en-es & Esta preocupaciÃ³n es mi principal preocupaciÃ³n en todos los <unk> y hay seÃ±ales que se <unk>          \\
nmt4\_en-es & Esta es mi principal preocupaciÃ³n en todos los <unk> y existen indicios de que esto vuelve a ocurrir <unk>       \\ \midrule

src:        & In order to achieve that, it is necessary to promote research, a step the rapporteur also proposes for the Mediterranean in his report. \\
ref:        & Para ello es necesario reforzar la investigaciÃ³n, como tambiÃ©n propone el ponente en su texto para el MediterrÃ¡neo.               \\
nmt3\_en-es & Para alcanzar el <unk> es necesario que se <unk> un paso <unk> el ponente <unk> tambiÃ©n el desarrollo del MediterrÃ¡neo en su <unk>          \\
nmt4\_en-es & Para <unk> es necesario promover el <unk> un paso que propone tambiÃ©n el <unk>       \\ \bottomrule


\end{tabular}
\end{table*}



\section{Analysis}

We compare our results with the ones obtained by article proposed by Luong \cite{luong17}. The models used by
Luong were trained on a dataset were Source language was vietnamite and Target was English. They trained
with 12K steps and got a Bleu Score of 25.5.\\

As we see in Table \ref{tab:2}, our best Bleu score is 19.6 in Model 4. Note that this model finish its training
 it took almost 8 hours training on over 400,000 sentences.\\
 
 Even though, we note there is still occurrences of the unkwon character '<unk>'. But with increaing training time,
 we also note its decrease.\\
 
 Sentences in our samples show their correctness with reference to our target translated sentences. Although, it
 is not exact, they show the correlation not to words but more surprisingly phrases.\\

As we can observe, it seems to be a relation between our metrics and training time and vocabulary size, beeing the
more important the first one.

\section{Conclusions}

Our changes in architecture dropped a series of interesting features on the nmt model used
in this work.\\

We did not mention earlier, but all of our models used the Scaled Luong Attention models. That is mainly
because the bleu scores got

This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.


%\end{document}  % This is where a 'short' article might terminate


% training details
